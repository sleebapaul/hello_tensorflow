{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "I'm prospecting this tutorial to be fun and straight forward. No jargon will be entertained / practiced without prior and proper explanation. But I think it would be up-and-coming if the reader has basic knowledge in Linear Algebra and Python.\n",
    "\n",
    "Let's begin with importing tensorflow library, shall we ? \n",
    "\n",
    "[Installation of tensorflow](https://www.tensorflow.org/install/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is TensorFlow and how it works ?\n",
    "\n",
    "TensorFlow is an open source software library for numerical computation, originally developed by researchers and engineers working on the Google Brain Team within Google's Machine Intelligence research organization. The flexible architecture of TensorFlow allows us to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API.\n",
    "\n",
    "What is this flexible architecture worth talking about?  \n",
    "\n",
    "Every computation performed using Tensorflow will be done in a two-steps structure:\n",
    "\n",
    "1. Define a computation graph \n",
    "    - Contains a series of operations (```ops```)\n",
    "    - Operations could be simple mathematical operations to complex numerical optimization techniques.\n",
    "2. Launch a Session for executing the graph \n",
    "    - Translates and passes the ops represented in graphs to the computation devices eg. GPU\n",
    "\n",
    "Too much new terms right? `Session` , `Graph` ... Don't worry, we'll cover all of it in detail :D\n",
    "\n",
    "Let's begin with operations in TensorFlow. \n",
    "\n",
    "As we introduced above, an operation could be anything. Addition, multiplication, calculating gradients ... Even the declaration of operands, e.g. Variables, are also operations in that sense. \n",
    "\n",
    "We'll begin with `source operations` in `tf` and then elaborate the ideas of `tensors, graph, session, variables and placeholders` as we go on.\n",
    "\n",
    "#### Source operations\n",
    "\n",
    "- Operations which can be used to pass information to other operations \n",
    "- Usually they don't take any input. \n",
    "    - Eg. tf.constant( [ ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(1, 1), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[2]])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be thinking why we're using a `source operation` in tf?   \n",
    "\n",
    "Simple answer is, we define `tensors` with them as you've seen just now. Now, the next obvious question is what is a `tensor`? \n",
    "\n",
    "We'll talk details about tensor in a moment. But, as of now, consider a `tensor` as more or less like an n-dimensional array. It has a shape, a data type and a value.\n",
    "\n",
    "Usually, when we define a variable or a constant in Python, immediately we get that value stored into that variable. Here comes the two-step procedure for computation in TensorFlow. As mentioned earlier, in `tf` just definition of an operation is not enough. Because it's just a computational graph and no value is asserted to it. If we need to load value into it, then a Session should be launched to execute that graph. Let's do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha!! We've a value now for `a`. It's 2. Now you see the shape of the tensor. It is (1,1). What does it mean? Let's try more examples to learn it yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tensor definition:  Tensor(\"Const_13:0\", shape=(1,), dtype=int32)\n",
      "Value on session:\n",
      "[2]\n",
      "\n",
      "Tensor definition:  Tensor(\"Const_14:0\", shape=(1, 1), dtype=int32)\n",
      "Value on session:\n",
      "[[2]]\n",
      "\n",
      "Tensor definition:  Tensor(\"Const_15:0\", shape=(2, 1), dtype=int32)\n",
      "Value on session:\n",
      "[[2]\n",
      " [1]]\n",
      "\n",
      "Tensor definition:  Tensor(\"Const_16:0\", shape=(1, 2), dtype=int32)\n",
      "Value on session:\n",
      "[[2 1]]\n",
      "\n",
      "Tensor definition:  Tensor(\"Const_17:0\", shape=(2, 2), dtype=int32)\n",
      "Value on session:\n",
      "[[2 1]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "a = tf.constant([2])\n",
    "print(\"\\nTensor definition: \", a)\n",
    "print(\"Value on session:\")\n",
    "print(sess.run(a))\n",
    "a = tf.constant([[2]])\n",
    "print(\"\\nTensor definition: \", a)\n",
    "print(\"Value on session:\")\n",
    "print(sess.run(a))\n",
    "a = tf.constant([[2], [1]])\n",
    "print(\"\\nTensor definition: \", a)\n",
    "print(\"Value on session:\")\n",
    "print(sess.run(a))\n",
    "a = tf.constant([[2, 1]])\n",
    "print(\"\\nTensor definition: \", a)\n",
    "print(\"Value on session:\")\n",
    "print(sess.run(a))\n",
    "a = tf.constant([[2, 1], [3, 4]])\n",
    "print(\"\\nTensor definition: \", a)\n",
    "print(\"Value on session:\")\n",
    "print(sess.run(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You got it right. A shape is actually the dimensions of that tensor. `No. of rows x No. of columns`\n",
    "\n",
    "Note that in first example, we are not specifying no. of columns and shape is displaying it accordingly.   \n",
    "\n",
    "`dtype` tells us about the data type of the variable and it is `int32`. \n",
    "\n",
    "Now let's perform an simple addition in tf. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor c:  Tensor(\"Add_2:0\", shape=(1, 1), dtype=int32)\n",
      "Value of c:  [[5]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[2]])\n",
    "b = tf.constant([[3]])\n",
    "c = tf.add(a, b)\n",
    "print(\"Tensor c: \", c)\n",
    "sess = tf.Session()\n",
    "print(\"Value of c: \", sess.run(c))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational graph  \n",
    "\n",
    "Let's discuss the Computational Graph we've just created for the sum operation.\n",
    "A computational graph consists of two parts\n",
    "\n",
    "1. `Edges` which are `tensors`.\n",
    "    - We have two `edges` here.\n",
    "        - a and b\n",
    "    - Through `edges` the `operations` are communicated.\n",
    "2. `Nodes` which are operations.\n",
    "    - tf.add( ) and definitions of a and b. \n",
    "\n",
    "Can you imagine how our graph would look like ?  \n",
    "\n",
    "![Alt Text](https://media.licdn.com/mpr/mpr/AAEAAQAAAAAAAAkjAAAAJDgzYTUxMGI3LTZjYTctNDliNi1hMDQ3LWU3YzNlYTE3ODU4Zg.png)\n",
    "\n",
    "This is a simple graph demonstrated for sum operation. But graphs get really complicated when we scale up. The graph below is used for a simple image classification problem using [Convolutional Neural Nets](http://cs231n.github.io/convolutional-networks/).\n",
    "\n",
    "![Alt Text](https://renatocunha.com/img/dlnd-image-classification/main-graph.png)\n",
    "\n",
    "\n",
    "I didn't bring up this graph to spread panic :D I wanted to give an idea about how a real problem is solved using Tensorflow. Afterall, Tensorflow is not made for performing trifle sums. We may use [Tensorboard](https://www.tensorflow.org/get_started/summaries_and_tensorboard) to visualize the computation graph.\n",
    "\n",
    "Now let's get into the detailed understanding of tensors.\n",
    "\n",
    "## Tensors \n",
    "\n",
    "Tensor has a latin origin with a simple meaning, `that which strecthes`. \n",
    "Tensor is a mathematical object that was originally used to analyze the way materials stretch under pressure. But nowadays we treat tensors as multidimensional arrays. \n",
    "\n",
    "Wait a sec. We need to talk about dimensions in Physics before we start reading Gospel of Tensors. :D\n",
    "\n",
    "Consider the following. \n",
    "\n",
    "![Alt Text](https://ibm.box.com/shared/static/ymn0hl3hf8s3xb4k15v22y5vmuodnue1.svg)\n",
    "\n",
    "- Zero dimension :- A point [A scalar]\n",
    "- One dimension :- A line [1-D array]\n",
    "- Two dimension :- A surface [2-D array]\n",
    "- Three dimension :- A volume [3-D array]\n",
    "- What about a four dimensional space ??\n",
    "    - It is bit difficult to visualize a 4-D space.\n",
    "    - Scientists call it a hyperspace or spacetime (4th axis being time).\n",
    "    - Maybe be we can think of a **volume changing in time** as a 4-D vector. \n",
    "    - It is complicated like some relationships. :P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scalar: \n",
      " [2]\n",
      "\n",
      "Vector: \n",
      " [2 3 4]\n",
      "\n",
      "Matrix: \n",
      " [[ 2  3  4]\n",
      " [ 5  6  7]\n",
      " [ 8  9 10]]\n",
      "\n",
      "Tensor: \n",
      " [[[ 1  2  3]\n",
      "  [ 5  6  7]\n",
      "  [ 8  9 10]]\n",
      "\n",
      " [[ 3  2  1]\n",
      "  [ 1 10 11]\n",
      "  [ 6  5  7]]\n",
      "\n",
      " [[ 9  7  6]\n",
      "  [18  4  3]\n",
      "  [ 7  5  4]]]\n"
     ]
    }
   ],
   "source": [
    "scalar = tf.constant([2])\n",
    "vector = tf.constant([2, 3, 4])\n",
    "matrix = tf.constant([[2, 3, 4], [5, 6, 7], [8, 9, 10]])\n",
    "tensor = tf.constant([[[1,2,3], [5, 6, 7], [8, 9, 10]], \n",
    "                      [[3, 2, 1], [1, 10, 11], [6, 5 ,7]], \n",
    "                      [[9, 7 , 6], [18, 4, 3], [7, 5, 4]]])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"\\nScalar: \\n\", sess.run(scalar))\n",
    "    print(\"\\nVector: \\n\", sess.run(vector))\n",
    "    print(\"\\nMatrix: \\n\", sess.run(matrix))\n",
    "    print(\"\\nTensor: \\n\", sess.run(tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Tensors ?\n",
    "    \n",
    "A Tensor gives a lot of freedom when it comes to structuring the dataset the way we would like to. No, I' m not making this up. Let's consider the example of an image. \n",
    "\n",
    "An image is represented digitally as pixels and each pixel is represented by a number called `pixel value`. \n",
    "\n",
    "For example a grayscale image, the pixel value is a single number that represents the brightness of the pixel. The most common  pixel format is the byte image, where this number is stored as an 8-bit integer giving a range of possible values from 0 to 255 (2^8). Typically zero is taken to be black, and 255 is taken to be white. Values in between make up the different shades of gray.\n",
    "\n",
    "![Alt Text](https://ibm.box.com/shared/static/xlpv9h5xws248c09k1rlx7cer69y4grh.png)\n",
    "\n",
    "To represent color images, separate red, green and blue components must be specified for each pixel (assuming an RGB colorspace), and so the `pixel value` is actually a vector of three numbers. Often the three different components are stored as three separate grayscale images known as color planes (one for each of red, green and blue), which have to be recombined when displaying or processing.\n",
    "\n",
    "Now an RGB image will be represented as a tensor as three 3 x 3 matrices stacked as we've seen above. \n",
    "\n",
    "Taking it forward, why don't we perform some matrix operations with **`tf`**, shall we? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matrix addition \n",
      "\n",
      "[[ 4  4  6]\n",
      " [ 8 10  6]\n",
      " [14 14 18]]\n",
      "\n",
      "Matrix multiplication \n",
      "\n",
      "[[ 32  30  30]\n",
      " [ 74  69  66]\n",
      " [116 108 102]]\n",
      "\n",
      "Elementwise matrix multiplication\n",
      "\n",
      "[[ 3  4  9]\n",
      " [16 25  0]\n",
      " [49 48 81]]\n"
     ]
    }
   ],
   "source": [
    "matrix_a = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "matrix_b = tf.constant([[3, 2, 3], [4, 5, 0], [7, 6, 9]])\n",
    "\n",
    "mat_sum = tf.add(matrix_a, matrix_b)\n",
    "mat_pro = tf.matmul(matrix_a, matrix_b)\n",
    "mat_pro_elem = tf.multiply(matrix_a, matrix_b)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"\\nMatrix addition \\n\")\n",
    "    print(sess.run(mat_sum))\n",
    "    print(\"\\nMatrix multiplication \\n\")\n",
    "    print(sess.run(mat_pro))\n",
    "    print(\"\\nElementwise matrix multiplication\\n\")\n",
    "    print(sess.run(mat_pro_elem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More power to `tf` : Variables\n",
    "\n",
    "- We can define a variable in a computation graph using **`tf.Variable()`** but that's not enough unlike `source operations`. \n",
    "- To deploy them in a `session` we should initialize all the defined variables before running the session. \n",
    "- **`tf.global_variables_initializer`** can do this job for us. \n",
    "\n",
    "Note:  \n",
    "Variables save a value during and after the calculations are executed which means there's no change in the value stored. This idea is really important when we understand `placeholders` after sometime.\n",
    "\n",
    "Let's implement a simple counter using what we know yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph\n",
    "state = tf.Variable(0) # define a variable\n",
    "step = tf.constant(1) # define a unit step\n",
    "new_value = tf.add(state, step) # define an operation \n",
    "update = tf.assign(state, new_value) # update the value of state to new_value using tf.assign()\n",
    "\n",
    "# Session\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(update))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the above code, **`tf`** will gift you a `FailedPreconditionError: Attempting to use uninitialized value Variable_2` error.   \n",
    "\n",
    "What is wrong here?   \n",
    "Attempting to use an uninitialized value is the crime we've committed here it says. So far we played around with `tf.constant()` but now a `tf.Variable()` is in the scene and it's going to make a difference. We'll be needing the initialization all the variables before using them in a **`tf.session()`**. This is called lazy evaluation, which means, evaluation of our variables/graph happens only at a session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Graph\n",
    "state = tf.Variable(0) # define a variable\n",
    "step = tf.constant(1) # define a unit step\n",
    "new_value = tf.add(state, step) # define an operation \n",
    "update = tf.assign(state, new_value) # update the value of state to new_value using tf.assign()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(sess.run(update))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woo-Hoo !! It works now. But you may think that, for implementing a simple counter, why do we need to go through all these procedures. Well, plucking a nail from the wall using an excavator would fit a good example to this situation. As we said earlier, `tf` is not meant to do trifle computations and as things scale up, we'll be more procedural and cautious.  \n",
    "\n",
    "\n",
    "\n",
    "## Placeholders\n",
    "\n",
    "Placeholders are say, variables itself, but won't receive a value until a later point or runtime. What do we mean by that ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = tf.placeholder(tf.int8)\n",
    "op = tf.multiply(sample, 10)\n",
    "print(\"Placeholder : \", sample)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(op))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the above code, you'll be confronting an `InvalidArgumentError`. Why?  \n",
    "\n",
    "Because the placeholder is not fed by any value. It's an empty space with a datatype.   \n",
    "\n",
    "We can compare placeholder with a reserving a hotel room. \n",
    "\n",
    " - room_221B = marriot.reservation ( male )\n",
    " \n",
    "Here, we're expecting a person of type `male` to fill that reservation at room number `221B` during the check-in at `Marriot` hotel. Until then, the room is empty.  \n",
    "\n",
    "Similarly,  \n",
    "\n",
    "- sample = tf.placeholder ( tf.int8 )\n",
    "\n",
    "We're expecting a data of type `tf.int8` to fill that empty space **`sample`** during the `session`. This is really important, as it enables us to feed data to a TensorFlow model from outside a model. \n",
    "\n",
    "To pass the data to the model we call the session with an extra argument `feed_dict` in which we should pass a dictionary with each placeholder name folowed by its respective data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholder :  Tensor(\"Placeholder_2:0\", shape=(3,), dtype=int8)\n",
      "[20 30 40]\n"
     ]
    }
   ],
   "source": [
    "sample = tf.placeholder(tf.int8, (3,))\n",
    "op = tf.multiply(sample, 10)\n",
    "print(\"Placeholder : \", sample)\n",
    "dictionary = {sample: [2, 3, 4]}\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(op, feed_dict=dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what are these placeholders and what do they do?   \n",
    "- Placeholders can be seen as \"holes\" in your model, \"holes\" which you will pass the data to. \n",
    "- We can create them using `tf.placeholder (datatype, shape)` \n",
    "- Datatype specifies the type of data (integers, floating points, strings, booleans) along with its precision (8, 16, 32, 64) bits. \n",
    "- Shape of the expected array can also be initialized during the intialization of a placeholder along with the datatype. It is optional. \n",
    "\n",
    "There is a detailed table given above regarding the various datatypes. The definition of each data type with the respective python syntax is defined as:  \n",
    "\n",
    "|Data type\t|Python type|Description|\n",
    "| --------- | --------- | --------- |\n",
    "|DT_FLOAT\t|tf.float32\t|32 bits floating point.|\n",
    "|DT_DOUBLE\t|tf.float64\t|64 bits floating point.|\n",
    "|DT_INT8\t|tf.int8\t|8 bits signed integer.|\n",
    "|DT_INT16\t|tf.int16\t|16 bits signed integer.|\n",
    "|DT_INT32\t|tf.int32\t|32 bits signed integer.|\n",
    "|DT_INT64\t|tf.int64\t|64 bits signed integer.|\n",
    "|DT_UINT8\t|tf.uint8\t|8 bits unsigned integer.|\n",
    "|DT_STRING\t|tf.string\t|Variable length byte arrays. Each element of a Tensor is a byte array.|\n",
    "|DT_BOOL\t|tf.bool\t|Boolean.|\n",
    "|DT_COMPLEX64\t|tf.complex64\t|Complex number made of two 32 bits floating points: real and imaginary parts.|\n",
    "|DT_COMPLEX128\t|tf.complex128\t|Complex number made of two 64 bits floating points: real and imaginary parts.|\n",
    "|DT_QINT8\t|tf.qint8\t|8 bits signed integer used in quantized Ops.|\n",
    "|DT_QINT32\t|tf.qint32\t|32 bits signed integer used in quantized Ops.|\n",
    "|DT_QUINT8\t|tf.quint8\t|8 bits unsigned integer used in quantized Ops.\n",
    "Starting from `sourcing operations`, we reached `placeholders` as expected. We'll discuss `operations` breifly and end this introductory tutorial. \n",
    "\n",
    "## Operations\n",
    "\n",
    "Operations are nodes that represent the mathematical operations over the tensors on a graph. These are like functions in python but operate directly over tensors and each one does a specific thing.\n",
    "\n",
    "Eg. tf.matmul, tf.add, tf.nn.sigmoid, tf.nn.relu\n",
    "\n",
    "NB : tf.nn.sigmoid and tf.nn.relu are activiation functions, which needs more explanation. Chuck it for now. \n",
    "\n",
    "So let's build a graph using all the concepts we've learned yet, right? \n",
    "\n",
    "Consider the following equation, $$ y = Wx + b $$ \n",
    "\n",
    "We need to evaluate values of $y$ for different $x$ keeping $W$ and $b$ as constant. \n",
    "\n",
    "Let's break down the graph.  \n",
    "\n",
    "- Here we've two operations, a sum and a multiplication. So, there would be two nodes in our graph. \n",
    "- We need to define two variables i.e. $W$ and $b$\n",
    "- We need to define one placeholder for $x$ and feed the value of $x$ each time the graph is executed in a session.\n",
    "\n",
    "So our graph would be, \n",
    "\n",
    "![Alt Text](https://github.com/sleebapaul/hello_tensorflow/blob/master/images/simple_graph.jpg)\n",
    "\n",
    "Now we'll code it using TensorFlow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.24850464]\n",
      " [ 1.05696559]\n",
      " [-0.9978447 ]\n",
      " [ 0.38015342]\n",
      " [ 1.77992475]\n",
      " [ 1.84230733]\n",
      " [ 0.64847338]\n",
      " [ 0.40530998]\n",
      " [ 0.92679751]\n",
      " [ 0.78063881]]\n",
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "# Graph\n",
    "\n",
    "# b is a tensor of dimension 10 x 1 with all values as 1\n",
    "b = tf.Variable(tf.ones((10, 1)))\n",
    "# W is a tensor with random but uniformly distributed values between -1 and 1 on a dimension of 10 x 10. \n",
    "W = tf.Variable(tf.random_uniform((10, 10), -1, 1)) \n",
    "# x is a placeholder for a 10 x 1 tensor with type float32. \n",
    "x = tf.placeholder(tf.float32, (10, 1))\n",
    "# calculating f using the above variables\n",
    "f = tf.add(tf.matmul(W, x), b) # tf.matmul() performs matrix multiplication\n",
    "\n",
    "# Session\n",
    "\n",
    "import numpy as np\n",
    "sess = tf.Session() # define session \n",
    "sess.run(tf.global_variables_initializer()) # initialize all the variable to be used\n",
    "output = sess.run(f, feed_dict={x: np.random.random_sample((10, 1))}) # evaluating f by feeding the \n",
    "print(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think this is good enough to begin with TensorFlow. As we move along the learning path, we would be able to see a lot of cool stuff. I hope you enjoyed this joy ride. Thanks :) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
